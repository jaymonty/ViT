{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f6cba4-89af-4e1f-8b7e-4edce3e0eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vit Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "886af955-b5f9-4880-83e7-f717505d9abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all messages, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "# For CUDA GEMMs determinism (must be set before torch import)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"   # best choice for A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55adc55f-76db-47a6-9a6a-ce636359ce86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755893284.705332   69578 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755893284.712007   69578 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755893284.728585   69578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755893284.728604   69578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755893284.728606   69578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755893284.728608   69578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from clearml import Task, OutputModel\n",
    "from datetime import datetime\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, silhouette_score, davies_bouldin_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network   import MLPClassifier\n",
    "from collections import Counter\n",
    "\n",
    "# Modular Files\n",
    "from data_loader import get_dataloaders, load_dataset_files\n",
    "from transformer_model import ViTForSimCLR, MAEModule\n",
    "from utils import CFG    #, zvm_thresholds_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb5c9a5-48da-4849-8aa9-92dcb581ed1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8efe81c-5cf8-4d60-ab14-6ab983b32327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For deterministic purposes\n",
    "def set_seed(seed=seed):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True, warn_only=False)\n",
    "\n",
    "    # Deterministic attention path\n",
    "    torch.backends.cuda.enable_flash_sdp(False)\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "    torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "    # Ampere precision determinism\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    torch.backends.cudnn.allow_tf32 = False\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"highest\")\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106492a7-3fa5-4ad3-844f-fa7729437cbf",
   "metadata": {},
   "source": [
    "Tier 1 = strict + reproducible: \n",
    "\n",
    "I enable deterministic algorithms, disable TF32 and fast attention, and set the cuBLAS workspace so results are bit-stable on the same setup; if I need invariance across num_workers, I also use index-keyed per-sample seeding. I use Tier 1 for ablations I care about, bug repros (NaNs/collapse), and final numbers/demos. \n",
    "\n",
    "I won't implement a Tier 0 (less strict) because training time is not negatively impacted going from 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79b9a0a-3f74-42ac-8947-72dc2b6b7b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det-algos: True\n",
      "TF32 (matmul/cudnn): False False\n",
      "SDPA flash/mem/math: False False True\n",
      "CUBLAS_WORKSPACE_CONFIG: :4096:8\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tier 1 should show: \n",
    "    det-algos: True, \n",
    "    TF32 both False, \n",
    "    SDPA flash=False, \n",
    "    mem=False, \n",
    "    math=True, \n",
    "    CUBLAS_WORKSPACE_CONFIG=\":4096:8\"\n",
    "'''\n",
    "\n",
    "print(\"det-algos:\", torch.are_deterministic_algorithms_enabled())\n",
    "print(\"TF32 (matmul/cudnn):\", torch.backends.cuda.matmul.allow_tf32,\n",
    "                              torch.backends.cudnn.allow_tf32)\n",
    "print(\"SDPA flash/mem/math:\",\n",
    "      torch.backends.cuda.flash_sdp_enabled(),\n",
    "      torch.backends.cuda.mem_efficient_sdp_enabled(),\n",
    "      torch.backends.cuda.math_sdp_enabled())\n",
    "print(\"CUBLAS_WORKSPACE_CONFIG:\", os.environ.get(\"CUBLAS_WORKSPACE_CONFIG\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80d862b-9ba1-4bfb-8585-7e417aa5d17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For dataloader / to be deterministic   \n",
    "def worker_init_fn(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac08db6-b6fd-4241-9a13-52e2d04887d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation set need deterministic generators\n",
    "g_train = torch.Generator().manual_seed(seed)\n",
    "g_val   = torch.Generator().manual_seed(seed + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf82f8a-260c-4257-bf27-aecf022cb648",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# Check that CUDA is available\n",
    "assert torch.cuda.is_available(), \"CUDA not available!\"\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "print(\"Using GPU:\", torch.cuda.get_device_name(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6ed30c7-9779-4f4e-9ab1-ca2578a69311",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed Check] Python random: 81\n",
      "[Seed Check] NumPy: 51\n",
      "[Seed Check] PyTorch: 42\n",
      "[Seed Check] PyTorch CUDA: 43\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Seed Check] Python random: {random.randint(0, 100)}\")\n",
    "print(f\"[Seed Check] NumPy: {np.random.randint(0, 100)}\")\n",
    "print(f\"[Seed Check] PyTorch: {torch.randint(0, 100, (1,)).item()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[Seed Check] PyTorch CUDA: {torch.randint(0, 100, (1,), device='cuda').item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "799e5f5b-a9c0-4380-92f2-0f48f33355df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from clearml import Dataset\n",
    "# import os\n",
    "\n",
    "# # Sanity Check data\n",
    "# ds = Dataset.get(dataset_id=\"d00ed3a421684bbfa96c03b77bc8ae98\")\n",
    "# local_copy = ds.get_local_copy()\n",
    "\n",
    "# print(\"Dataset downloaded to:\", local_copy)\n",
    "# print(\"Files in dataset:\")\n",
    "# print(os.listdir(local_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4468a00f-680d-4f47-9aa7-855c9620238b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base = \"/home/jupyter/.clearml/cache/storage_manager/datasets/ds_d00ed3a421684bbfa96c03b77bc8ae98\"\n",
    "\n",
    "# spectrograms = np.load(os.path.join(base, \"spectrograms.npy\"))\n",
    "# persistence = np.load(os.path.join(base, \"persistence_spectra.npy\"))\n",
    "# labels = np.load(os.path.join(base, \"labels.npy\"))\n",
    "# label_encoder = np.load(os.path.join(base, \"label_encoder.npz\"))['classes']\n",
    "\n",
    "# print(\"Spectrograms shape:\", spectrograms.shape)\n",
    "# print(\"Persistence shape:\", persistence.shape)\n",
    "# print(\"Labels shape:\", labels.shape)\n",
    "# print(\"Classes:\", label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2236beb6-e81d-46d0-9524-ff0278d35bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spectrograms.npy\n",
      "Loading persistence_spectra.npy\n",
      "Loading labels.npy\n",
      "Loading metadata.npz\n",
      "Loading label_encoder.npz\n",
      "Final class names: ['AR_drone', 'Bebop_drone', 'Phantom', 'ambient', 'mavic_pro_2']\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# === Load Data and Model ====\n",
    "# =============================\n",
    "data = load_dataset_files(\"d00ed3a421684bbfa96c03b77bc8ae98\")\n",
    "\n",
    "#====================================================\n",
    "# Pretrain loaders (with SimCLR augmentations)\n",
    "train_loader, val_loader, class_names = get_dataloaders(\n",
    "    data,\n",
    "    # batch_size=BATCH_SIZE,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    # resize_to=IMAGE_SIZE,\n",
    "    resize_to=CFG.IMAGE_SIZE,\n",
    "    augment=True,\n",
    "    val_split=0.1,\n",
    "    # num_workers=NUM_WORKERS,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    generator=g_train,\n",
    "    #generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "# Eval loaders (no augmentations) for linear‐probe & clustering metrics\n",
    "train_eval_loader, val_eval_loader, _ = get_dataloaders(\n",
    "    data,\n",
    "    # batch_size=BATCH_SIZE,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    # resize_to=IMAGE_SIZE,\n",
    "    resize_to=CFG.IMAGE_SIZE,\n",
    "    augment=False,\n",
    "    val_split=0.3,\n",
    "    # num_workers=NUM_WORKERS,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    generator=g_val,\n",
    "   # generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "#====================================================\n",
    "# Debug print to verify\n",
    "print(\"Final class names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc3dfa0-3cd8-4f27-ac5c-df708e148c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current \n",
    "H, W = (CFG.IMAGE_SIZE, CFG.IMAGE_SIZE) if isinstance(CFG.IMAGE_SIZE, int) else CFG.IMAGE_SIZE\n",
    "P = int(CFG.PATCH_SIZE)\n",
    "n_h, n_w = H // P, W // P\n",
    "N = n_h * n_w\n",
    "\n",
    "# baseline at 256 image/16 patch\n",
    "BASE_H = int(getattr(CFG, \"BASELINE_IMAGE_SIZE\", 256))\n",
    "BASE_W = int(getattr(CFG, \"BASELINE_IMAGE_WIDTH\", BASE_H))\n",
    "BASE_P = int(getattr(CFG, \"BASELINE_PATCH_SIZE\", 16))\n",
    "N0 = (BASE_H // BASE_P) * (BASE_W // BASE_P)\n",
    "\n",
    "# + 1 for CLS token\n",
    "attn_mult = float(((N + 1) / (N0 + 1)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be4b8e7a-d7ce-4754-a6d3-d36a07fb5194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=de0f4b207bee4a38a173d87cd403a981\n",
      "ClearML results page: https://app.clear.ml/projects/8d94fdfcc93849c699ecfc70878d8dc3/experiments/de0f4b207bee4a38a173d87cd403a981/output/log\n",
      "ClearML results page: https://app.clear.ml/projects/8d94fdfcc93849c699ecfc70878d8dc3/experiments/de0f4b207bee4a38a173d87cd403a981/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# === ClearML Setup ===\n",
    "# =========================\n",
    "task = Task.init(\n",
    "    project_name=\"Signal Fingerprinting/ViT Fingerprinting\",\n",
    "    task_name=\"ViT Training_94\",\n",
    "    auto_connect_frameworks={\"pytorch\": False}\n",
    ")\n",
    "task.add_tags(['Full Dataset',f'Lambda={CFG.LAMBDA_MAE}',f'Temp={CFG.TEMPERATURE}'])\n",
    "\n",
    "logger = task.get_logger()\n",
    "\n",
    "task_params = {\n",
    "    \"Epochs\":               CFG.EPOCHS,\n",
    "    \"Base learning_rate\":   CFG.BASE_LR,           \n",
    "    \"Encoder learning_rate\": CFG.ENCODER_LR,       \n",
    "    \"Projector learning_rate\": CFG.PROJECTOR_LR,   \n",
    "    \"Eta min\":              CFG.ETA_MIN,\n",
    "    \"Weight decay\":         CFG.WEIGHT_DECAY,\n",
    "    \"Warmup fraction\":      CFG.WARMUP_FRAC,\n",
    "    \"Warmup epochs\":        CFG.WARMUP_EPOCHS,\n",
    "    \"Starting Lambda MAE\":  CFG.LAMBDA_MAE,\n",
    "    \"Temperature\":          CFG.TEMPERATURE,\n",
    "    \"Num_workers\":          CFG.NUM_WORKERS,\n",
    "    \"Device\":               CFG.DEVICE,\n",
    "    \"ViT Embedding Size\":   CFG.EMBED_DIM,\n",
    "    \"Attention Heads\":      CFG.NUM_HEADS,\n",
    "    \"ViT Layers\":           CFG.DEPTH,\n",
    "    \"Batch Size\":           CFG.BATCH_SIZE,\n",
    "    \"Patch Size\":           CFG.PATCH_SIZE,\n",
    "    \"Image Size\":           CFG.IMAGE_SIZE,\n",
    "    \"Projection Head Hidden Size |Hidden Dim|\": CFG.HIDDEN_DIM,\n",
    "    \"Projection Head Output Size |Projection Dim|\": CFG.PROJECTION_DIM,\n",
    "    \"Dataset_id\":           \"d00ed3a421684bbfa96c03b77bc8ae98\",\n",
    "    \"Class Names\":          list(class_names),\n",
    "}\n",
    "task.connect(task_params)\n",
    "\n",
    "# =========== token budget =============\n",
    "task.connect({\n",
    "    \"Patches (h×w)\": f\"{n_h}×{n_w}\",\n",
    "    \"Tokens (N)\": int(N),\n",
    "    \"Baseline\": f\"{BASE_H}×{BASE_W}/{BASE_P}  (N0={N0})\",\n",
    "    \"Attn cost vs baseline\": attn_mult,\n",
    "}, name=\"token_budget\")\n",
    "# =======================================\n",
    "\n",
    "with open(\"transformer_model.py\", \"r\") as f:\n",
    "    model_def = f.read()\n",
    "\n",
    "model_config = {\n",
    "    \"serial\": f\"{datetime.now().strftime('%H:%M:%S')}\",\n",
    "    \"framework\": \"pytorch\",\n",
    "    \"args\": {\n",
    "        \"embedding_dim\": CFG.EMBED_DIM,\n",
    "        \"projection_dim\": CFG.PROJECTION_DIM\n",
    "    },\n",
    "    \"input_signature\": f\"[B, 6, {CFG.IMAGE_SIZE}, {CFG.IMAGE_SIZE}]\",\n",
    "    \"output_signature\": f\"[B, {CFG.EMBED_DIM}] + [B, {CFG.PROJECTION_DIM}]\",\n",
    "    \"def\": model_def\n",
    "}\n",
    "\n",
    "output_model = OutputModel(task=task, name=\"ViT Transformer\", config_dict=model_config)\n",
    "output_model.set_upload_destination(\"gs://ewa-clearml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f20cab03-e83f-4c68-b6c7-12ea93f973f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class distribution:\n",
      "AR_drone       : 162 samples\n",
      "Bebop_drone    : 168 samples\n",
      "Phantom        : 390 samples\n",
      "ambient        : 10104 samples\n",
      "mavic_pro_2    : 597 samples\n"
     ]
    }
   ],
   "source": [
    "# Get underlying dataset from Subset\n",
    "subset: torch.utils.data.Subset = train_loader.dataset\n",
    "full_dataset = subset.dataset  # This is SimCLRDualViewDataset\n",
    "base_dataset = full_dataset.base_dataset  # This is RFSpectrogramDataset\n",
    "\n",
    "# Get labels\n",
    "labels = base_dataset.labels\n",
    "\n",
    "# Count \n",
    "class_counts = Counter(labels)\n",
    "print(\"Training class distribution:\")\n",
    "for class_id, count in sorted(class_counts.items()):\n",
    "    class_name = class_names[class_id] if class_names else str(class_id)\n",
    "    print(f\"{class_name:15}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1905a5-f1b2-43ea-bd05-65a249343fac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:382: UserWarning:\n",
      "\n",
      "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ViT model and MAE \n",
    "\n",
    "# needed to pass from utils.py\n",
    "model = ViTForSimCLR(\n",
    "    in_channels=6,\n",
    "    img_size=CFG.IMAGE_SIZE,\n",
    "    patch_size=CFG.PATCH_SIZE,\n",
    "    emb_dim=CFG.EMBED_DIM,\n",
    "    depth=CFG.DEPTH,\n",
    "    num_heads=CFG.NUM_HEADS,\n",
    "    proj_hidden=CFG.HIDDEN_DIM,     \n",
    "    proj_out=CFG.PROJECTION_DIM,    \n",
    ").to(CFG.DEVICE)\n",
    "\n",
    "mae_module = MAEModule().to(DEVICE)\n",
    "\n",
    "# Adjust encoder and projector LR\n",
    "# This gives the SimCLR head much larger steps (PROJECTOR_LR = BASE_LR) \n",
    "# while keeping the ViT backbone more conservative (ENCODER_LR = BASE_LR × 0.1)  / check util.py for setting\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.encoder.parameters(),   'lr': CFG.ENCODER_LR,   'weight_decay': CFG.WEIGHT_DECAY},\n",
    "    {'params': model.projector.parameters(), 'lr': CFG.PROJECTOR_LR, 'weight_decay': 0.0},\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.EPOCHS, eta_min=CFG.ETA_MIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "656d1b21-68c1-454d-a8d0-efd58102ebea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# ViTForSimCLR expects 6-channel \n",
    "arch = summary(\n",
    "    model,\n",
    "    input_size=(1, 6, CFG.IMAGE_SIZE, CFG.IMAGE_SIZE),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    "    depth=30   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9edc281-f0ce-4af7-90e1-91f890c03056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #\n",
      "=============================================================================================================================\n",
      "ViTForSimCLR                                       [1, 6, 252, 252]          [1, 128]                  --\n",
      "├─PatchEmbed: 1-1                                  [1, 6, 252, 252]          [1, 324, 128]             --\n",
      "│    └─Conv2d: 2-1                                 [1, 6, 252, 252]          [1, 128, 18, 18]          150,656\n",
      "├─ViTEncoder: 1-2                                  [1, 324, 128]             [1, 325, 128]             41,728\n",
      "│    └─Dropout: 2-2                                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    └─TransformerEncoder: 2-3                     [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    └─ModuleList: 3-1                        --                        --                        --\n",
      "│    │    │    └─TransformerEncoderLayer: 4-1      [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─MultiheadAttention: 5-1      [1, 325, 128]             [1, 325, 128]             66,048\n",
      "│    │    │    │    └─Dropout: 5-2                 [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-3               [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    │    └─Linear: 5-4                  [1, 325, 128]             [1, 325, 512]             66,048\n",
      "│    │    │    │    └─Dropout: 5-5                 [1, 325, 512]             [1, 325, 512]             --\n",
      "│    │    │    │    └─Linear: 5-6                  [1, 325, 512]             [1, 325, 128]             65,664\n",
      "│    │    │    │    └─Dropout: 5-7                 [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-8               [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    └─TransformerEncoderLayer: 4-2      [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─MultiheadAttention: 5-9      [1, 325, 128]             [1, 325, 128]             66,048\n",
      "│    │    │    │    └─Dropout: 5-10                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-11              [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    │    └─Linear: 5-12                 [1, 325, 128]             [1, 325, 512]             66,048\n",
      "│    │    │    │    └─Dropout: 5-13                [1, 325, 512]             [1, 325, 512]             --\n",
      "│    │    │    │    └─Linear: 5-14                 [1, 325, 512]             [1, 325, 128]             65,664\n",
      "│    │    │    │    └─Dropout: 5-15                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-16              [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    └─TransformerEncoderLayer: 4-3      [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─MultiheadAttention: 5-17     [1, 325, 128]             [1, 325, 128]             66,048\n",
      "│    │    │    │    └─Dropout: 5-18                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-19              [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    │    └─Linear: 5-20                 [1, 325, 128]             [1, 325, 512]             66,048\n",
      "│    │    │    │    └─Dropout: 5-21                [1, 325, 512]             [1, 325, 512]             --\n",
      "│    │    │    │    └─Linear: 5-22                 [1, 325, 512]             [1, 325, 128]             65,664\n",
      "│    │    │    │    └─Dropout: 5-23                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-24              [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    └─TransformerEncoderLayer: 4-4      [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─MultiheadAttention: 5-25     [1, 325, 128]             [1, 325, 128]             66,048\n",
      "│    │    │    │    └─Dropout: 5-26                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-27              [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    │    └─Linear: 5-28                 [1, 325, 128]             [1, 325, 512]             66,048\n",
      "│    │    │    │    └─Dropout: 5-29                [1, 325, 512]             [1, 325, 512]             --\n",
      "│    │    │    │    └─Linear: 5-30                 [1, 325, 512]             [1, 325, 128]             65,664\n",
      "│    │    │    │    └─Dropout: 5-31                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-32              [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    └─TransformerEncoderLayer: 4-5      [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─MultiheadAttention: 5-33     [1, 325, 128]             [1, 325, 128]             66,048\n",
      "│    │    │    │    └─Dropout: 5-34                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-35              [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    │    └─Linear: 5-36                 [1, 325, 128]             [1, 325, 512]             66,048\n",
      "│    │    │    │    └─Dropout: 5-37                [1, 325, 512]             [1, 325, 512]             --\n",
      "│    │    │    │    └─Linear: 5-38                 [1, 325, 512]             [1, 325, 128]             65,664\n",
      "│    │    │    │    └─Dropout: 5-39                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-40              [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    └─TransformerEncoderLayer: 4-6      [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─MultiheadAttention: 5-41     [1, 325, 128]             [1, 325, 128]             66,048\n",
      "│    │    │    │    └─Dropout: 5-42                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-43              [1, 325, 128]             [1, 325, 128]             256\n",
      "│    │    │    │    └─Linear: 5-44                 [1, 325, 128]             [1, 325, 512]             66,048\n",
      "│    │    │    │    └─Dropout: 5-45                [1, 325, 512]             [1, 325, 512]             --\n",
      "│    │    │    │    └─Linear: 5-46                 [1, 325, 512]             [1, 325, 128]             65,664\n",
      "│    │    │    │    └─Dropout: 5-47                [1, 325, 128]             [1, 325, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-48              [1, 325, 128]             [1, 325, 128]             256\n",
      "├─ProjectionHead: 1-3                              [1, 128]                  [1, 128]                  --\n",
      "│    └─Sequential: 2-4                             [1, 128]                  [1, 128]                  --\n",
      "│    │    └─Linear: 3-2                            [1, 128]                  [1, 512]                  66,048\n",
      "│    │    └─BatchNorm1d: 3-3                       [1, 512]                  [1, 512]                  1,024\n",
      "│    │    └─ReLU: 3-4                              [1, 512]                  [1, 512]                  --\n",
      "│    │    └─Linear: 3-5                            [1, 512]                  [1, 128]                  65,664\n",
      "│    │    └─BatchNorm1d: 3-6                       [1, 128]                  [1, 128]                  256\n",
      "=============================================================================================================================\n",
      "Total params: 1,515,008\n",
      "Trainable params: 1,515,008\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 49.74\n",
      "=============================================================================================================================\n",
      "Input size (MB): 1.52\n",
      "Forward/backward pass size (MB): 14.32\n",
      "Params size (MB): 4.31\n",
      "Estimated Total Size (MB): 20.15\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ad4ce6d-6e8d-4b9d-8349-a0400389dbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAEModule(\n",
      "  (enc_to_dec): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (decoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MAE module parameters: 1,612,800\n"
     ]
    }
   ],
   "source": [
    "# print the MAE module’s sub-module structure\n",
    "print(mae_module)\n",
    "\n",
    "# total number of parameters\n",
    "total = sum(p.numel() for p in mae_module.parameters())\n",
    "print(f\"MAE module parameters: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0f2554a-cead-4474-be31-e5bf72bdda2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# === SimCLR Contrastive Loss ===\n",
    "# =============================\n",
    "def contrastive_loss(z1, z2, temperature=0.5):\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    N = z1.shape[0]\n",
    "    z = torch.cat([z1, z2], dim=0)  # [2N, D]\n",
    "\n",
    "    # Cosine similarity matrix\n",
    "    sim_matrix = torch.matmul(z, z.T) / temperature  # [2N, 2N]\n",
    "\n",
    "    # Create mask to exclude self-similarities\n",
    "    mask = torch.eye(2 * N, device=z.device).bool()\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, -1e9)\n",
    "\n",
    "    # Positive pairs are [i, i+N] and [i+N, i]\n",
    "    positives = torch.cat([torch.arange(N, 2 * N), torch.arange(0, N)]).to(z.device)\n",
    "\n",
    "    # Labels: each sample's positive is at index `positives[i]`\n",
    "    loss = F.cross_entropy(sim_matrix, positives)\n",
    "    \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dbffc44-0d0b-43d8-a495-a20348200a72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step   0 | SimCLR loss: 30.5340\n",
      "group 0 lr = 1.00e-03\n",
      "group 1 lr = 1.00e-02\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step  10 | SimCLR loss: 33.0656\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step  20 | SimCLR loss: 31.7250\n",
      "group 0 lr = 1.00e-03\n",
      "group 1 lr = 1.00e-02\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step  30 | SimCLR loss: 27.2287\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step  40 | SimCLR loss: 25.7900\n",
      "group 0 lr = 1.00e-03\n",
      "group 1 lr = 1.00e-02\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step  50 | SimCLR loss: 21.9654\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step  60 | SimCLR loss: 17.7378\n",
      "group 0 lr = 1.00e-03\n",
      "group 1 lr = 1.00e-02\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step  70 | SimCLR loss: 13.3951\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step  80 | SimCLR loss: 9.9733\n",
      "group 0 lr = 1.00e-03\n",
      "group 1 lr = 1.00e-02\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Step  90 | SimCLR loss: 8.0603\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n",
      "Mean abs diff between views: 0.5014529824256897\n"
     ]
    }
   ],
   "source": [
    "# For troubleshooting simclr loss (sanity check, looks good)\n",
    "fixed = next(iter(train_loader))\n",
    "for step in range(100):\n",
    "    v1, v2, _ = fixed\n",
    "    diff = (v1 - v2).abs().mean().item()\n",
    "    print(\"Mean abs diff between views:\", diff)\n",
    "    \n",
    "    \n",
    "    v1, v2 = v1.to(DEVICE), v2.to(DEVICE)\n",
    "    z1, z2 = model(v1)[1], model(v2)[1]\n",
    "    \n",
    "    loss = contrastive_loss(z1, z2, CFG.TEMPERATURE)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:3d} | SimCLR loss: {loss.item():.4f}\")\n",
    "        \n",
    "    if step % 20 == 0:\n",
    "        for i, g in enumerate(optimizer.param_groups):\n",
    "            print(f\"group {i} lr = {g['lr']:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa8ae912-38ee-4546-a430-01d7f0d23772",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean abs diff between views: 0.5103262662887573\n"
     ]
    }
   ],
   "source": [
    "# For troubleshooting simclr loss (Sanity check too, looks good)\n",
    "v1, v2, _ = next(iter(train_loader))\n",
    "diff = (v1 - v2).abs().mean().item()\n",
    "print(\"Mean abs diff between views:\", diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8b3bab7-c9cc-4a0e-9354-4842d50bb874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_embeddings(loader, model, device):\n",
    "    model.eval()\n",
    "    feats, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for view1, view2, labels in loader:\n",
    "            x = view1.to(device)                      \n",
    "            cls_tok, _ = model(x)\n",
    "            feats.append(cls_tok.cpu().numpy())\n",
    "            labs.append(labels.numpy())\n",
    "    return np.vstack(feats), np.concatenate(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0f2051-4d0f-4095-ae18-0f7fe4df7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for computing var_z_mean for checking for collapse\n",
    "def emb_var_mean_from_numpy(X: np.ndarray, l2_normalize: bool = True):\n",
    "\n",
    "    if l2_normalize:\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n",
    "        X = X / norms\n",
    "    # \n",
    "    var_per_dim = X.var(axis=0, ddof=0)\n",
    "    mean_var = float(var_per_dim.mean())\n",
    "    return mean_var, var_per_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc28a5a5-ed3a-4cdf-9609-5e8dd15f2d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_linear_probe(model, train_loader, val_loader, device):\n",
    "    # Extract embeddings + labels\n",
    "    X_train, y_train = extract_embeddings(train_loader, model, device)\n",
    "    X_val,   y_val   = extract_embeddings(val_loader,   model, device)\n",
    "    \n",
    "#=========== This is for computing z_var_mean inside of linear probe instead of on its own ============\n",
    "    # ---- z_var_mean on CLS (L2-normalized) using the SAME X_val ----\n",
    "    zvm, var_per_dim = emb_var_mean_from_numpy(X_val, l2_normalize=True)\n",
    "    \n",
    "#======================================================================================================\n",
    "\n",
    "    # Train a simple logistic regression on train set    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "    \n",
    "    # To handle class imbalance, F1-score was 20.31%\n",
    "    probe = LogisticRegression(\n",
    "        max_iter=2_000, \n",
    "        random_state=seed,  # for repeatability\n",
    "    ).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Now using scaled for better performance on evaling \n",
    "    y_pred = probe.predict(X_val_scaled)\n",
    "    # Accuracy & F1 score / using macro for individual class performance\n",
    "    acc    = accuracy_score(y_val, y_pred)\n",
    "    f1  = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    # Clustering metrics on the *validation* embeddings\n",
    "    sil = silhouette_score(X_val_scaled, y_val)  \n",
    "    db  = davies_bouldin_score(X_val_scaled,   y_val)\n",
    "\n",
    "    # return acc,f1, sil, db, y_val, y_pred\n",
    "    return acc,f1, sil, db, y_val, y_pred, zvm, var_per_dim  # this is to return z_var_mean\n",
    "    # return acc,f1, sil, db, y_val, y_pred, zvm_cls, var_cls, zvm_proj, var_proj "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f853d688-7a16-4557-b9ca-95654aa9d299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MLP Probe \n",
    "def evaluate_mlp_probe(model, train_loader, val_loader, device):\n",
    "    # This is basically the same as linear probe, but swapping out for MLP\n",
    "    X_train, y_train = extract_embeddings(train_loader, model, device)\n",
    "    X_val,   y_val   = extract_embeddings(val_loader,   model, device)\n",
    "    # X_train, _, y_train = extract_embeddings_both(train_loader, model, device)\n",
    "    # X_val, _,  y_val   = extract_embeddings_both(val_loader,   model, device)\n",
    "\n",
    "    scaler     = StandardScaler().fit(X_train)\n",
    "    X_train_sc = scaler.transform(X_train)\n",
    "    X_val_sc   = scaler.transform(X_val)\n",
    "\n",
    "    # small 1-hidden-layer MLP / keep it simple\n",
    "    probe = MLPClassifier(\n",
    "        hidden_layer_sizes=(256,),    # one hidden layer of 256 units\n",
    "        activation='relu',            # ReLU non-linearity\n",
    "        alpha=1e-4,                   # L2 penalty on weights\n",
    "        max_iter=2_000,                 # bumping up to match log reg\n",
    "        random_state=seed,  # for repeatability\n",
    "    )\n",
    "    probe.fit(X_train_sc, y_train)\n",
    "    y_pred = probe.predict(X_val_sc)\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1  = f1_score(y_val, y_pred, average='macro')\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30053bee-f565-4f37-b997-a6b0a9c14a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_umap_plotly(emb_2d, y_int, epoch_idx, logger, class_names, title_prefix=\"UMAP of ViT [CLS] Embeddings\"):\n",
    "    \n",
    "    labels_text = [class_names[int(i)] for i in y_int]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"UMAP-1\": emb_2d[:, 0],\n",
    "        \"UMAP-2\": emb_2d[:, 1],\n",
    "        \"label\":  labels_text,\n",
    "    })\n",
    "    \n",
    "    palette = {\n",
    "        'AR_drone':   '#0072B2',  #blue\n",
    "        'Bebop_drone':'#E69F00',  # orange\n",
    "        'Phantom':    '#009E73',  # green\n",
    "        'mavic_pro_2':'#D55E00',  # light orange\n",
    "        'ambient':    '#7A7A7A',  # gray\n",
    "    }\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df, x=\"UMAP-1\", \n",
    "        y=\"UMAP-2\",\n",
    "        color=\"label\",                     \n",
    "        color_discrete_map=palette, # get rid of this for different classes / or not use color scheme\n",
    "        opacity=0.8,\n",
    "        title=f\"{title_prefix} — Epoch {epoch_idx}\"\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    fig.update_layout(legend_title_text=\"Class\")\n",
    "\n",
    "    logger.report_plotly(\n",
    "        title=title_prefix,\n",
    "        series=f\"Epoch {epoch_idx}\",\n",
    "        iteration=epoch_idx,\n",
    "        figure=fig\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48fe2ef2-0441-4ead-a721-7120182b69ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "UMAP_DIR = \"umap_images\"\n",
    "os.makedirs(UMAP_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ecdc840-fc6d-40c5-a566-dead7ee40187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEMPERATURE = CFG.TEMPERATURE\n",
    "EPOCHS = CFG.EPOCHS\n",
    "LAMBDA_MAE = CFG.LAMBDA_MAE\n",
    "\n",
    "# temperature state \n",
    "tau_current = float(CFG.TEMPERATURE)    \n",
    "TAU_MIN     = 1e-4                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd443c-2af8-42d7-8c35-7428e8255f11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/150\n",
      "Epoch 1/150 | Total: 6.5248 | SimCLR: 6.5166 | MAE: 0.8234\n",
      "[Linear Probe @ epoch 1]  Acc=0.9904, F1=0.8653, Sil=0.8046, DB=5.5595\n",
      "[MLP @ epoch 1]    Acc=0.9863, F1=0.7380\n",
      "[Z-var Mean @ epoch 1 |zvm < 1 ⚠️|] z_var_mean=2.4918e-05 | n=3426 | min=4.0419e-07, max=2.1776e-04\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AR_drone     0.6111    0.7333    0.6667        45\n",
      " Bebop_drone     0.7273    0.6038    0.6598        53\n",
      "     Phantom     1.0000    1.0000    1.0000       109\n",
      "     ambient     1.0000    1.0000    1.0000      3031\n",
      " mavic_pro_2     1.0000    1.0000    1.0000       188\n",
      "\n",
      "    accuracy                         0.9904      3426\n",
      "   macro avg     0.8677    0.8674    0.8653      3426\n",
      "weighted avg     0.9907    0.9904    0.9904      3426\n",
      "\n",
      "Epoch: 2/150\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# === Training Loop ===\n",
    "# =============================\n",
    "best_combined_loss = float('inf')\n",
    "patience, patience_counter = 20, 0  # Early stopping was 7, now 15\n",
    "embeddings_list, labels_list = [], []\n",
    "best_f1 = 0.0\n",
    "lambda_mae = LAMBDA_MAE\n",
    "#===============================\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, total_simclr, total_mae = 0, 0, 0\n",
    "    \n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for view1, view2, labels in train_loader:\n",
    "        view1, view2 = view1.to(DEVICE), view2.to(DEVICE)\n",
    "\n",
    "        # Get both CLS token and projection head outputs\n",
    "        z1_cls, z1_proj = model(view1)  # z1_cls: [B, 128], z1_proj: [B, 64]\n",
    "        z2_cls, z2_proj = model(view2)\n",
    "\n",
    "        # SimCLR contrastive loss (use projected features)\n",
    "        simclr_loss = contrastive_loss(z1_proj, z2_proj, temperature=tau_current)\n",
    "\n",
    "        # Masked Autoencoder (MAE) loss using view1\n",
    "        recon_loss = mae_module(view1, model)\n",
    "\n",
    "        # Total loss\n",
    "        # combined_loss = simclr_loss + LAMBDA_MAE * recon_loss\n",
    "        combined_loss = simclr_loss + lambda_mae * recon_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += combined_loss.item()\n",
    "        total_simclr += simclr_loss.item()\n",
    "        total_mae += recon_loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_simclr = total_simclr / len(train_loader)\n",
    "    avg_mae = total_mae / len(train_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # report to ClearML\n",
    "    logger.report_scalar(\"Combined Loss\",\"Combined Total\", avg_loss, epoch+1)\n",
    "    logger.report_scalar(\"SimCLR Loss\",\"SimCLR\", avg_simclr, epoch+1)\n",
    "    logger.report_scalar(\"MAE Loss\",\"MAE\", avg_mae, epoch+1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Total: {avg_loss:.4f} | SimCLR: {avg_simclr:.4f} | MAE: {avg_mae:.4f}\")\n",
    "    \n",
    "    # ===== every epoch do a linear-probe eval and UMAP & MLP-probe eval =====\n",
    "    if (epoch + 1) % 1 == 0:   \n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True) # added for oom\n",
    "        torch.cuda.empty_cache() # added to clean up oom\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        model.eval()    # added for probes\n",
    "        \n",
    "        # Added z_var_mean ability\n",
    "        acc, f1, sil, db, y_true, y_pred, zvm, var_per_dim = evaluate_linear_probe(\n",
    "            model, train_eval_loader, val_eval_loader, DEVICE\n",
    "        )\n",
    "        \n",
    "        print(f\"[Linear Probe @ epoch {epoch+1}]  Acc={acc:.4f}, F1={f1:.4f}, Sil={sil:.4f}, DB={db:.4f}\")\n",
    "\n",
    "        logger.report_scalar(\"Probe Accuracy\",\"Linear Accuracy\", acc, epoch+1)\n",
    "        logger.report_scalar(\"Probe F1-Score\",\"Linear F1-Score\", f1,  epoch+1)\n",
    "        logger.report_scalar(\"Linear Probe Silhouette Score\",\"Silhouette Score\",sil,epoch+1)\n",
    "        logger.report_scalar(\"Linear Probe DB Score\",\"Davies-Bouldin\", db, epoch+1)\n",
    "        logger.report_scalar(\"MAE Weight\", \"lambda_mae\", lambda_mae, epoch+1)\n",
    "        \n",
    "        \n",
    "        # --- MLP‐probe eval ---\n",
    "        acc_mlp, f1_mlp = evaluate_mlp_probe(\n",
    "            model, train_eval_loader, val_eval_loader, DEVICE\n",
    "        )\n",
    "        print(f\"[MLP @ epoch {epoch+1}]    Acc={acc_mlp:.4f}, F1={f1_mlp:.4f}\")\n",
    "        \n",
    "        logger.report_scalar(\"Probe Accuracy\",\"MLP Accuracy\", acc_mlp, epoch+1)\n",
    "        logger.report_scalar(\"Probe F1-Score\",\"MLP F1-Score\",  f1_mlp,  epoch+1)\n",
    "        \n",
    "        #--- Z-var Mean for Collapse ---\n",
    "        print(f\"[Z-var Mean @ epoch {epoch+1} |zvm < 1 ⚠️|] z_var_mean={zvm:.4e} | n={len(y_true)} | \"f\"min={var_per_dim.min():.4e}, max={var_per_dim.max():.4e}\")\n",
    "        \n",
    "        # ClearML logging \n",
    "        logger.report_scalar(\"Collapse Check |zvm < 1 ⚠️|\", \"Z Var Mean\", zvm * 1e4, epoch+1)\n",
    "             \n",
    "    \n",
    "        #=============== Save Model =====================\n",
    "        if f1 >= best_f1 + 1e-4:\n",
    "            \n",
    "            # ensure patience counter resets to prevent early stopping\n",
    "            patience_counter = 0\n",
    "            \n",
    "            cr_str = classification_report(\n",
    "                y_true, y_pred,\n",
    "                target_names=class_names,\n",
    "                digits=4,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(cr_str)\n",
    "\n",
    "            best_f1 = f1\n",
    "            # eval_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_f1_model.pth\")\n",
    "        \n",
    "            # Save model\n",
    "            best_path   = f\"best_model_epoch_{epoch+1}.pth\"\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "            \n",
    "            output_model.update_weights(\n",
    "                weights_filename=best_path,  \n",
    "                target_filename=f\"model_best_f1.pth\"\n",
    "            )\n",
    "            output_model.wait_for_uploads() \n",
    "\n",
    "        #================= UMAP =================================\n",
    "            # --- UMAP visualization on validation embeddings ---\n",
    "            X_val, y_val = extract_embeddings(val_eval_loader, model, DEVICE)\n",
    "            # X_val, _, y_val = extract_embeddings_both(val_eval_loader, model, DEVICE)\n",
    "            reducer      = umap.UMAP(n_neighbors=50, min_dist=0.2, metric='cosine')    # changed from 30 to 50 , 0.1 to 0.2\n",
    "            umap_emb     = reducer.fit_transform(X_val)  # shape [N,2]\n",
    "            \n",
    "            \n",
    "            #----------- Plotly approach for UMAP w/ labels -------------\n",
    "            # log interactive UMAP with class labels / made function to be cleaner\n",
    "            log_umap_plotly(umap_emb, y_val, epoch+1, logger, class_names)\n",
    "            \n",
    "    # Early stopping\n",
    "    if avg_loss < best_combined_loss:\n",
    "        best_combined_loss = avg_loss\n",
    "        patience_counter  = 0\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Patience Counter: {patience_counter}\")\n",
    "        \n",
    "        # when the early-stop count is at 10 OR GREATER, lower by 0.001 each epoch\n",
    "        if patience_counter >= 10:\n",
    "            \n",
    "            # stop using MAE / focus solely on SimCLR loss\n",
    "            lambda_mae = 0\n",
    "            \n",
    "            new_tau = max(tau_current - 0.001, TAU_MIN)\n",
    "            if new_tau != tau_current:\n",
    "                print(f\"Lowering temperature: {tau_current:.6f} → {new_tau:.6f} (patience={patience_counter})\")\n",
    "            tau_current = new_tau\n",
    "            # allow new temp to impact performance / then, if no improvement for 5 epochs it will decrease\n",
    "            patience_counter = 5\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "            \n",
    "    logger.report_scalar(\"SimCLR Temperature\", \"tau_current\", tau_current, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97db2b-7e36-4f59-8235-ef169f15f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================\n",
    "# # === Save Model Snapshot ====\n",
    "# # =============================\n",
    "# torch.save(model.state_dict(), \"vit_simclr.pth\")\n",
    "# output_model.update_weights(\"vit_simclr.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be26b41-5201-431a-80fa-6753e4f43df1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d29ee2-ae8a-4056-8fdc-00f6d8324f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix mae to not go to zero but be zero by end of epoch\n",
    "# undersample classes\n",
    "# scale ViT\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
